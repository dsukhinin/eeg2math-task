{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bfc2b3-3007-474f-b0d1-5a641d6bce33",
   "metadata": {},
   "source": [
    "## Train two models to classify EEG records into 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e7fbf-9222-4de8-a9de-93d3938b9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # SELECT GPU CONFIG HERE\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cec786-2c78-4801-bc4d-7e1374fda87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n channels in EEG \n",
    "N_CHANNELS = 28\n",
    "\n",
    "#n classes\n",
    "N_CLASSES = 3\n",
    "\n",
    "#n time steps per eeg file  1500 = 3 seconds of eeg \n",
    "ECOG_INT_LEN = 1500\n",
    "\n",
    "#subjects from 25 to 44\n",
    "\n",
    "subjects = [i for i in range(25, 45)]\n",
    "\n",
    "# will be exludeded from pretraining and used for finetuning of second model\n",
    "\n",
    "test_subj = [31]\n",
    "\n",
    "SPLIT = 0.66 # train/test for first model\n",
    "\n",
    "TEST_SPLIT = 0.66 # train/test of excluded subject(s) for second model\n",
    "\n",
    "#change here for something meaningfull\n",
    "TEST_N = 1\n",
    "\n",
    "RUN_NAME = f'eeg2math_test_{TEST_N}'\n",
    "\n",
    "# Select one of two baseline arhitecture for first model \n",
    "TEST_MODEL = 'resnet'\n",
    "#TEST_MODEL = 'ednet'\n",
    "\n",
    "\n",
    "# data dir \n",
    "DATASET_DIR = os.path.join(os.getcwd(), 'data/ds02_25-44_mix')\n",
    "\n",
    "subjects_to_process = [(os.path.join(DATASET_DIR, f'{i}_y.npy'), i) for i in subjects ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20369d-a3b7-4387-a798-b4fe72523636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uzip dataset if needed\n",
    "!tar -xf data/ds02_25-44_mix.zip -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81335030-6978-4605-bf73-afa2da4aa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating train/test sets \n",
    "\n",
    "train_smp = []\n",
    "\n",
    "train_smp_s = []\n",
    "\n",
    "test_smp = []\n",
    "\n",
    "test_smp_s = []\n",
    "\n",
    "tests_sep = []\n",
    "\n",
    "for drd, i in subjects_to_process:\n",
    "    \n",
    "    print(\"processing subject\", i)\n",
    "    \n",
    "    tmpY = np.load(drd, allow_pickle=True)\n",
    "    \n",
    "    tmpY = tmpY - 5 # 5,6,7 labels to 0, 1, 2\n",
    "    \n",
    "    sept = []\n",
    "    \n",
    "    for j in range(tmpY.shape[0]):\n",
    "        \n",
    "        if i in test_subj:\n",
    "            \n",
    "            if j < int(tmpY.shape[0]*TEST_SPLIT):\n",
    "                \n",
    "                train_smp_s.append((i,j,tmpY[j]))\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                test_smp_s.append((i,j,tmpY[j]))\n",
    "        else:\n",
    "            \n",
    "            if j < int(tmpY.shape[0]*SPLIT):\n",
    "                \n",
    "                train_smp.append((i,j,tmpY[j]))\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                test_smp.append((i,j,tmpY[j]))\n",
    "                sept.append((i,j,tmpY[j]))\n",
    "                \n",
    "    tests_sep.append(sept)\n",
    "    \n",
    "\n",
    "def read_one(smpl):\n",
    "    \n",
    "    i, j, k = smpl\n",
    "                         \n",
    "    y_f = k\n",
    "    \n",
    "    x_f = np.load(os.path.join(DATASET_DIR, f\"x_{i}_{j}.npy\"), allow_pickle=True)\n",
    "    \n",
    "    x_f = sklearn.preprocessing.scale(x_f)\n",
    "    \n",
    "    x_f = np.clip(x_f, -3, 3)\n",
    "    \n",
    "    return x_f.astype(\"float32\"), y_f.astype(\"int32\")\n",
    "    \n",
    "def preprocess(idx):\n",
    "    \n",
    "    spec, audio = tf.numpy_function(read_one, [idx], [tf.float32, tf.int32])\n",
    "    \n",
    "    return spec,  audio\n",
    "\n",
    "\n",
    "    \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_smp,))\n",
    "train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_smp,))\n",
    "test_dataset = test_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "sep_test_ds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa1225-fca0-4722-9179-5cbffcf1e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# baseline model resnet style 1d convs converted\n",
    "\n",
    "def resnet_18_1dconv(input_shape, model_type = 18, use_head = True):\n",
    "    \n",
    "    def res_block(x, n_filters):\n",
    "        \n",
    "        res = x \n",
    "        \n",
    "        x = layers.Conv1D(n_filters, 3, 2, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        x = layers.Conv1D(n_filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        res = layers.Conv1D(n_filters, 3, 2, padding=\"same\")(res)\n",
    "        res = layers.BatchNormalization()(res)\n",
    "        res = layers.Activation('relu')(res)\n",
    "        \n",
    "        x = layers.Add()([x, res])\n",
    "        \n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        return x \n",
    "        \n",
    "        \n",
    "    def iden_block(x, n_filters):\n",
    "    \n",
    "        res = x \n",
    "        \n",
    "        x = layers.Conv1D(n_filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        x = layers.Conv1D(n_filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Add()([x, res])\n",
    "        \n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        return x \n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(64, 7, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = iden_block(x, 64)\n",
    "    x = iden_block(x, 64)\n",
    "    \n",
    "    x = res_block(x, 128)\n",
    "    x = iden_block(x, 128)\n",
    "    \n",
    "    if model_type > 10:\n",
    "    \n",
    "        x = res_block(x, 256)\n",
    "        x = iden_block(x, 256)\n",
    "    \n",
    "    if model_type > 14:\n",
    "\n",
    "        x = res_block(x, 512)\n",
    "        x = iden_block(x, 512)\n",
    "        \n",
    "    \n",
    "    if use_head:\n",
    "    \n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "        outputs = layers.Dense(N_CLASSES, activation='linear')(x)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        outputs = x\n",
    "        \n",
    "    \n",
    "    return keras.Model(inputs, outputs, name='res_net')\n",
    "\n",
    "# 1d for baseline\n",
    "def resnet_10_lstm_mix(input_shape):\n",
    "    \n",
    "    DOWNSAMPLING =  10\n",
    "    \n",
    "    res10 = resnet_18_1dconv(input_shape, model_type = 10, use_head = False)\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = res10(inputs)\n",
    "    \n",
    "    x = x[:,::DOWNSAMPLING,:]\n",
    "    \n",
    "    x = layers.Bidirectional(layers.LSTM(80, return_sequences=True))(x)\n",
    "    \n",
    "    x = layers.Dropout(.1)(x)\n",
    "    \n",
    "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "    \n",
    "    outputs = layers.Dense(N_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='resnet10_lstm')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# envelope detector net from paper Petrosyan et al. 2022, Hyperparams selected from internal tests\n",
    "\n",
    "def ed_net(input_shape, n_branches = 88, lstm_units = 64, filtering_size = 45, envelope_size = 25):\n",
    "    DOWNSAMPLING =  10\n",
    "    FILTERING_SIZE = filtering_size\n",
    "    ENVELOPE_SIZE = envelope_size\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(n_branches, 1, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "    \n",
    "    x = layers.Conv1D(n_branches, FILTERING_SIZE, padding=\"same\", groups=n_branches, use_bias = False)(x)\n",
    "    x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "    x = layers.LeakyReLU(-1)(x)\n",
    "    \n",
    "    x = layers.Conv1D(n_branches, ENVELOPE_SIZE, padding=\"same\",  groups=n_branches)(x)\n",
    "    \n",
    "    x = x[:,::DOWNSAMPLING,:]\n",
    "    \n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units//2))(x)\n",
    "    \n",
    "    x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "    \n",
    "    outputs = layers.Dense(N_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name=f'ednet_{n_branches}_{lstm_units}_f_{filtering_size}_e_{envelope_size}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756f572-75b0-4af2-844b-439f3e018995",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL == 'resnet':\n",
    "    \n",
    "    model = resnet_10_lstm_mix((ECOG_INT_LEN, N_CHANNELS))\n",
    "    \n",
    "elif TEST_MODEL == 'ednet':\n",
    "    \n",
    "    model = ed_net((ECOG_INT_LEN, N_CHANNELS))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f9088-dcf4-4382-b2ac-4198dd15e37e",
   "metadata": {},
   "source": [
    "## Train first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356a3c2-7b96-4ff0-aedb-d91c7190acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "weight_decay = 0.00001\n",
    "\n",
    "LR_RATE = 0.0001 \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "N_EPOCH = 200 # 100 - 200  \n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=LR_RATE, weight_decay=weight_decay)\n",
    "\n",
    "lfn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "msca = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "\n",
    "checkpoint_filepath = './models/'+model.name+'_'+RUN_NAME+'_best.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss= lfn, metrics=msca)\n",
    "\n",
    "hist = model.fit(train_dataset.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), \n",
    "          validation_data = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), \n",
    "          epochs=N_EPOCH,\n",
    "          callbacks=[model_checkpoint_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf2d20-0a59-4140-8339-90f8941da2df",
   "metadata": {},
   "source": [
    "## Evaluating first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79e476-2a1c-4e1b-86ca-455c3dab3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading best weights\n",
    "model.load_weights('./models/'+model.name+'_'+RUN_NAME+'_best.h5')\n",
    "\n",
    "\n",
    "res = model.evaluate(test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "sep_res = []\n",
    "\n",
    "for tst in tests_sep:\n",
    "    \n",
    "    if len(tst) == 0:\n",
    "        continue\n",
    "    \n",
    "    sep_test_dataset = tf.data.Dataset.from_tensor_slices((tst,))\n",
    "    sep_test_dataset = sep_test_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    sres = model.evaluate(sep_test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
    "    sep_res.append(sres[1])\n",
    "    \n",
    "print(model.name+'_'+RUN_NAME, f'{res[1]:.3f}')    \n",
    "    \n",
    "for sub, rs in zip(subjects, sep_res):\n",
    "    print(f\"subject {sub} = {rs:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d9e93-2934-4ce9-bd19-a9933cfd4b1b",
   "metadata": {},
   "source": [
    "## Converting first model into fuature extrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37c6c5-6846-4891-b40a-0b65011560c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = keras.Model(model.inputs, model.layers[-2].output, name=f'lmodel')\n",
    "lmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d86cdc-884f-4caa-ac91-02388cb23c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting EEG to vectors\n",
    "train_data = []\n",
    "train_gts = []\n",
    "\n",
    "for smp in train_dataset.as_numpy_iterator():\n",
    "    x, y = smp\n",
    "    if y != 1:\n",
    "        train_data.append(np.squeeze(lmodel.predict(np.expand_dims(x, axis=0), verbose=0)))\n",
    "        train_gts.append(np.clip(y,0,1))\n",
    "\n",
    "print(len(train_data), len(train_gts), train_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574fc03-1609-4a51-bae2-799da8620f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_gts = []\n",
    "\n",
    "for smp in test_dataset.as_numpy_iterator():\n",
    "    x, y = smp\n",
    "    if y != 1:\n",
    "        val_data.append(np.squeeze(lmodel.predict(np.expand_dims(x, axis=0), verbose=0)))\n",
    "        val_gts.append(np.clip(y,0,1))\n",
    "\n",
    "print(len(val_data), len(val_gts), val_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eda4e0-6662-4b89-a838-a48612acb491",
   "metadata": {},
   "source": [
    "## Data preprocessing for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83047fb-30e1-4e4a-a2fc-b63a1ae390dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of vectors in sequence for second model\n",
    "INTV = 5 \n",
    "\n",
    "def read_one_train(smpl):\n",
    "                         \n",
    "    y_f = train_gts[smpl]\n",
    "    \n",
    "    x_f = train_data[smpl-INTV:smpl]\n",
    "    \n",
    "    return np.array(x_f).astype(\"float32\"), y_f.astype(\"int32\")\n",
    "    \n",
    "def lpreprocess_train(idx):\n",
    "    \n",
    "    spec, audio = tf.numpy_function(read_one_train, [idx], [tf.float32, tf.int32])\n",
    "    \n",
    "    return spec,  audio\n",
    "\n",
    "def read_one_test(smpl):\n",
    "                         \n",
    "    y_f = val_gts[smpl]\n",
    "    \n",
    "    x_f = val_data[smpl-INTV:smpl]\n",
    "    \n",
    "    return np.array(x_f).astype(\"float32\"), y_f.astype(\"int32\")\n",
    "    \n",
    "def lpreprocess_test(idx):\n",
    "    \n",
    "    spec, audio = tf.numpy_function(read_one_test, [idx], [tf.float32, tf.int32])\n",
    "    \n",
    "    return spec,  audio\n",
    "\n",
    "\n",
    "t_samples = np.arange(INTV, len(train_data))\n",
    "v_samples = np.arange(INTV, len(val_data))\n",
    "\n",
    "ltrain_dataset = tf.data.Dataset.from_tensor_slices((t_samples,))\n",
    "ltrain_dataset = ltrain_dataset.map(lpreprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "ltest_dataset = tf.data.Dataset.from_tensor_slices((v_samples,))\n",
    "ltest_dataset = ltest_dataset.map(lpreprocess_test, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8200912-12b8-40ae-a4e7-88356c151b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple second model for baseline\n",
    "\n",
    "def temporal_class_mix(input_shape):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(128, input_shape[0], padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    return keras.Model(inputs , outputs, name=f'temp_bclass_cnn_rnn')\n",
    "    \n",
    "    \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c53221-043a-43fc-8e80-c5df2e2f78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model =  temporal_class_mix((INTV, 64)) \n",
    "\n",
    "tmp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d8912-2da4-4d7c-a5de-a961ec0b7afe",
   "metadata": {},
   "source": [
    "## Training second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c73e2b-eab7-466d-8e13-4c537680b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "weight_decay = 0.00001\n",
    "\n",
    "LR_RATE = 0.0001\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "N_EPOCH = 50\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=LR_RATE, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "lfn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "msca =  [\n",
    "      #tf.keras.metrics.TruePositives(name='tp'),\n",
    "      #tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      #tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      #tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), \n",
    "]\n",
    "\n",
    "checkpoint_filepath = './models/'+tmp_model.name+'_'+RUN_NAME+'_best.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "tmp_model.compile(optimizer=optimizer, loss= lfn, metrics=msca)\n",
    "\n",
    "hist = tmp_model.fit(ltrain_dataset.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), \n",
    "          validation_data = ltest_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), \n",
    "          epochs=N_EPOCH,\n",
    "          callbacks=[model_checkpoint_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620bc54-a038-4577-92af-ec1683fd726f",
   "metadata": {},
   "source": [
    "## Evaluating second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2d311-adad-472c-8f65-7f47d9d3c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model.load_weights('./models/'+tmp_model.name+'_'+RUN_NAME+'_best.h5')\n",
    "ev_res = tmp_model.evaluate(ltest_dataset.batch(BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe02869-34c1-4918-8cda-f6444dde4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p, normalize='true')\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a2ea6-322f-4311-89c0-027cea6dccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tmp_model.predict(ltest_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "plot_cm(val_gts[INTV:], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2484055-d59c-4735-8e35-9c11409652a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.plot(preds > 0.5)\n",
    "plt.plot(val_gts[INTV:])\n",
    "\n",
    "plt.legend([\"Model prediction\", \"GT\"])\n",
    "plt.title(\"Predicted vs GT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96b7e4-8715-49cb-afce-90afb9f148d2",
   "metadata": {},
   "source": [
    "## Preparing latent vectors for second model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d351e8-1be6-40ff-ab83-5d0d71f9593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set of excluded subject\n",
    "train_dataset1 = tf.data.Dataset.from_tensor_slices((train_smp_s,))\n",
    "train_dataset1 = train_dataset1.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# test set of excluded subject\n",
    "test_dataset1 = tf.data.Dataset.from_tensor_slices((test_smp_s,))\n",
    "test_dataset1 = test_dataset1.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99301d3b-e751-475e-882c-6246fb0d7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = []\n",
    "train_gts1 = []\n",
    "\n",
    "for smp in train_dataset1.as_numpy_iterator():\n",
    "    x, y = smp\n",
    "    if y != 1:\n",
    "        train_data1.append(np.squeeze(lmodel.predict(np.expand_dims(x, axis=0), verbose=0)))\n",
    "        train_gts1.append(np.clip(y, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4a91a-2bab-46c4-93ac-0b64b48e4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data1 = []\n",
    "val_gts1 = []\n",
    "for smp in test_dataset1.as_numpy_iterator():\n",
    "    x, y = smp\n",
    "    if y != 1:\n",
    "        val_data1.append(np.squeeze(lmodel.predict(np.expand_dims(x, axis=0), verbose=0)))\n",
    "        val_gts1.append(np.clip(y, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca086b-1c92-441f-9dbd-7bfcf573e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_train(smpl):\n",
    "                         \n",
    "    y_f = train_gts1[smpl]\n",
    "    \n",
    "    x_f = train_data1[smpl-INTV:smpl]\n",
    "    \n",
    "    return np.array(x_f).astype(\"float32\"), y_f.astype(\"int32\")\n",
    "    \n",
    "def lpreprocess_train(idx):\n",
    "    \n",
    "    spec, audio = tf.numpy_function(read_one_train, [idx], [tf.float32, tf.int32])\n",
    "    \n",
    "    return spec,  audio\n",
    "\n",
    "\n",
    "\n",
    "def read_one_test(smpl):\n",
    "                         \n",
    "    y_f = val_gts1[smpl]\n",
    "    \n",
    "    x_f = val_data1[smpl-INTV:smpl]\n",
    "    \n",
    "    return np.array(x_f).astype(\"float32\"), y_f.astype(\"int32\")\n",
    "    \n",
    "def lpreprocess_test(idx):\n",
    "    \n",
    "    spec, audio = tf.numpy_function(read_one_test, [idx], [tf.float32, tf.int32])\n",
    "    \n",
    "    return spec,  audio\n",
    "\n",
    "\n",
    "\n",
    "t_samples1 = np.arange(INTV, int(len(train_data1)))\n",
    "\n",
    "v_samples1 = np.arange(INTV, len(val_data1))\n",
    "\n",
    "\n",
    "ltrain_dataset1 = tf.data.Dataset.from_tensor_slices((t_samples1,))\n",
    "ltrain_dataset1 = ltrain_dataset1.map(lpreprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "ltest_dataset1 = tf.data.Dataset.from_tensor_slices((v_samples1,))\n",
    "ltest_dataset1 = ltest_dataset1.map(lpreprocess_test, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7a6c3-7997-44fe-953b-3bab207bdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model.load_weights('./models/'+tmp_model.name+'_'+RUN_NAME+'_best.h5')\n",
    "ev_res = tmp_model.evaluate(ltest_dataset1.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bae24-9fc8-4910-bfc6-b2422f62f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing naive second model ...\n",
    "preds1 = tmp_model.predict(ltest_dataset1.batch(BATCH_SIZE))\n",
    "plot_cm(val_gts1[INTV:], preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96239a50-8030-470f-af7f-42fd1b99a48e",
   "metadata": {},
   "source": [
    "## Finetunig second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306cee5-0232-487a-8a13-0ff3736bf1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.00001\n",
    "\n",
    "LR_RATE = 0.00001\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=LR_RATE, weight_decay=weight_decay)\n",
    "\n",
    "#optimizer = tf.optimizers.Adam(learning_rate=LR_RATE)\n",
    "\n",
    "lfn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "msca =  [\n",
    "      #tf.keras.metrics.TruePositives(name='tp'),\n",
    "      #tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      #tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      #tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "checkpoint_filepath = './models/'+tmp_model.name+'_'+RUN_NAME+'_ftune25_best.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "tmp_model.compile(optimizer=optimizer, loss= lfn, metrics=msca)\n",
    "\n",
    "hist = tmp_model.fit(ltrain_dataset1.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), \n",
    "          validation_data = ltest_dataset1.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), \n",
    "          epochs=100,\n",
    "          callbacks=[model_checkpoint_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1cfa4-de5d-4ea3-a17a-d9d8df98f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval of finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929216c-f8e3-468c-bcef-1a8128831ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model.load_weights('./models/'+tmp_model.name+'_'+RUN_NAME+'_ftune25_best.h5')\n",
    "preds100 = tmp_model.predict(ltest_dataset1.batch(BATCH_SIZE))\n",
    "plot_cm(val_gts1[INTV:], preds100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de302cef-1d93-4147-bad7-78fe20db78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.plot(preds100 > 0.5)\n",
    "plt.plot(val_gts1[INTV:])\n",
    "\n",
    "plt.legend([\"Model prediction\", \"GT\"])\n",
    "plt.title(\"Predicted vs GT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec8158-91ee-472d-93b3-d6206b152e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = tmp_model.evaluate((ltest_dataset1.batch(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1fb38-f2f1-4da3-8fd0-4ecbda4bbae7",
   "metadata": {},
   "source": [
    "## Testing moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb618e-5813-41b2-ae37-2d1dae9a41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 10\n",
    "\n",
    "cumsum_vec = np.cumsum(preds100 > 0.5) \n",
    "ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(ma_vec>0.5)\n",
    "plt.plot(val_gts1[INTV:])\n",
    "\n",
    "plt.legend([\"Model prediction\", \"GT\"])\n",
    "plt.title(\"Predicted vs GT\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
